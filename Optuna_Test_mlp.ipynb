{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MikJak75/ResearchCopies/blob/main/Optuna_Test_mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itUDW97QQsFO"
      },
      "outputs": [],
      "source": [
        "#Defining a Multilayer Perceptron, MLP.\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self,dim1,dim2):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(dim1*dim2, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 16),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(16, 1),\n",
        "      nn.ReLU()\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Forward pass'''\n",
        "    return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.optim import optimizer\n",
        "### Define the loss function \n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "### Define the learning rate\n",
        "lr= 0.001\n",
        "\n",
        "### Set the random seed for reproducible results\n",
        "torch.manual_seed(0)\n",
        "\n",
        "### Initialize dimensions\n",
        "in_d1 = x_train.shape[2]\n",
        "in_d2 = x_train.shape[3]\n",
        "\n",
        "#model \n",
        "model = MLP(dim1=in_d1,dim2=in_d2)\n",
        "params_to_optimize = [\n",
        "    {'params': model.parameters()}\n",
        "]\n",
        "\n",
        "#Define the optimizer\n",
        "optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n",
        "\n",
        "# Check if the GPU is available\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f'Selected device: {device}')\n",
        "# Move model to device\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "BSmEC0P2QuCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 100\n",
        "for epoch in range(n_epochs):\n",
        "    # monitor losses\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "    \n",
        "#Train the model \n",
        "    model.train() \n",
        "    for data,label in train_loader:\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "        optim.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_fn(output,label)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        # train_loss += loss.item() * data.size(0)\n",
        "        train_loss += loss"
      ],
      "metadata": {
        "id": "DxFysDDuQ2Iu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}